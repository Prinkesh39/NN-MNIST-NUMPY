{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaSmKzdpdpcGUKlDadSCHE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "aLqy3c6ssado"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"X_train shape: \", X_train.shape)\n",
        "print (\"y_train shape: \", y_train.shape)\n",
        "print (\"X_test shape: \", X_test.shape)\n",
        "print (\"y_test shape: \", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff72cbb-67b8-4123-f3c9-db3d791d3231",
        "id": "51yQREp5sad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape:  (60000, 28, 28)\n",
            "y_train shape:  (60000,)\n",
            "X_test shape:  (10000, 28, 28)\n",
            "y_test shape:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_enco(data):\n",
        "  '''Convert the dataset into one_hot_encoding'''\n",
        "  #creating a matrix of the same shape as of data with zeros\n",
        "  one_hot_data = np.zeros((len(data), len(np.unique(data))))\n",
        "  for index, values in enumerate(data):\n",
        "    one_hot_data[index][values] = 1\n",
        "  return one_hot_data  "
      ],
      "metadata": {
        "id": "qNVzR_zPsad7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshaping sample of our input train data\n",
        "images = (X_train[0:1000].reshape(1000, 28*28) / 255)\n",
        "labels = y_train[0:1000]\n",
        "\n",
        "#one_hot encoding labels\n",
        "labels = one_hot_enco(labels)"
      ],
      "metadata": {
        "id": "If3j5cvksad-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshapeing our test data\n",
        "test_images = (X_test.reshape(len(X_test), 28*28) / 255)\n",
        "test_labels = one_hot_enco(y_test)\n"
      ],
      "metadata": {
        "id": "7VTgHj3wsaeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (images.shape, test_images.shape)\n",
        "print (labels.shape, test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262875ce-2324-41aa-ac36-cb52e3d939d6",
        "id": "iWF6PdjBsaeC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 784) (10000, 784)\n",
            "(1000, 10) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining some functions to introduce non-linearity\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def softmax(x):\n",
        "  return (np.exp(x) / np.sum(np.exp(x), axis = 1, keepdims = True))\n",
        "\n",
        "def tanh_deriv(x):\n",
        "  return 1 - (x ** 2)"
      ],
      "metadata": {
        "id": "C2uUFfrYSDay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialising weights parameter\n",
        "W01 = 0.02 * np.random.random((784, 100)) - 0.01\n",
        "W12 = 0.2 * np.random.random((100, 10)) - 0.1"
      ],
      "metadata": {
        "id": "B9Xile4KslFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "alpha, iterations, batch_size = 0.005, 300, 100\n",
        "for j in range(iterations):\n",
        "  correct_cnt = 0\n",
        "  for i in range(int(len(images) / batch_size)): #Using Batch Gradient descent\n",
        "    batch_start, batch_end = (i * batch_size), ((i+1) * batch_size)\n",
        "    #passing one example of length 784 pixels at a time\n",
        "    X = images[batch_start: batch_end] \n",
        "    #Applying forward propagation\n",
        "    layer_1 = tanh(np.dot(X, W01))#Output of hidden layer\n",
        "    #We will employ dropout regularisation technique to avoid overfitting\n",
        "    dropout = np.random.randint(2, size = layer_1.shape)\n",
        "    layer_1 *= dropout * 2 #Multiply by 2 as half of the nodes are turned off \n",
        "    layer_2 = softmax(np.dot(layer_1, W12))\n",
        "\n",
        "    for k in range(batch_size):\n",
        "      correct_cnt += int(np.argmax(labels[batch_start+k:batch_start+k+1])\\\n",
        "                         == np.argmax(layer_2[k:k+1]))\n",
        "    #Applying backward propagation\n",
        "    layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
        "    layer_1_delta = layer_2_delta.dot(W12.T) * tanh_deriv(layer_1)\n",
        "    layer_1_delta *= dropout\n",
        "\n",
        "    W12_delta = layer_1.T.dot(layer_2_delta)\n",
        "    W01_delta = X.T.dot(layer_1_delta)\n",
        "\n",
        "    #Updating weight parameters\n",
        "    W01 += alpha * W01_delta\n",
        "    W12 += alpha * W12_delta\n",
        "#Applying the forward propagation in test dataset with best parameters\n",
        "  test_correct_cnt = 0\n",
        "  for i in range(len(test_images)):\n",
        "    X_test = test_images[i:i+1]\n",
        "    layer_1 = tanh(np.dot(X_test, W01))\n",
        "    layer_2 = np.dot(layer_1, W12)\n",
        "    test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
        "#Let's print the accuracy for both training and test data per iteration\n",
        "  if j % 10 == 0:\n",
        "    sys.stdout.write (\"\\n\" + \"Iterations: \" + str(j) + \" Training Accuracy: \" +\\\n",
        "                      str(correct_cnt / float(len(images))) +\\\n",
        "                      \" Testing Accuracy: \" + \\\n",
        "                      str(test_correct_cnt / float(len(test_images))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H3wTYKUvB0z",
        "outputId": "014fc8bf-6b1e-436c-f2cc-c7d1a7ee43e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iterations: 0 Training Accuracy: 0.882 Testing Accuracy: 0.8297\n",
            "Iterations: 10 Training Accuracy: 0.885 Testing Accuracy: 0.8308\n",
            "Iterations: 20 Training Accuracy: 0.884 Testing Accuracy: 0.8318\n",
            "Iterations: 30 Training Accuracy: 0.879 Testing Accuracy: 0.8345\n",
            "Iterations: 40 Training Accuracy: 0.883 Testing Accuracy: 0.8359\n",
            "Iterations: 50 Training Accuracy: 0.889 Testing Accuracy: 0.8372\n",
            "Iterations: 60 Training Accuracy: 0.893 Testing Accuracy: 0.8384\n",
            "Iterations: 70 Training Accuracy: 0.885 Testing Accuracy: 0.8391\n",
            "Iterations: 80 Training Accuracy: 0.883 Testing Accuracy: 0.8402\n",
            "Iterations: 90 Training Accuracy: 0.897 Testing Accuracy: 0.8415\n",
            "Iterations: 100 Training Accuracy: 0.897 Testing Accuracy: 0.8421\n",
            "Iterations: 110 Training Accuracy: 0.899 Testing Accuracy: 0.843\n",
            "Iterations: 120 Training Accuracy: 0.892 Testing Accuracy: 0.8436\n",
            "Iterations: 130 Training Accuracy: 0.898 Testing Accuracy: 0.8444\n",
            "Iterations: 140 Training Accuracy: 0.899 Testing Accuracy: 0.8458\n",
            "Iterations: 150 Training Accuracy: 0.898 Testing Accuracy: 0.846\n",
            "Iterations: 160 Training Accuracy: 0.906 Testing Accuracy: 0.8471\n",
            "Iterations: 170 Training Accuracy: 0.9 Testing Accuracy: 0.8479\n",
            "Iterations: 180 Training Accuracy: 0.912 Testing Accuracy: 0.8481\n",
            "Iterations: 190 Training Accuracy: 0.908 Testing Accuracy: 0.8489\n",
            "Iterations: 200 Training Accuracy: 0.907 Testing Accuracy: 0.8498\n",
            "Iterations: 210 Training Accuracy: 0.917 Testing Accuracy: 0.8503\n",
            "Iterations: 220 Training Accuracy: 0.913 Testing Accuracy: 0.8512\n",
            "Iterations: 230 Training Accuracy: 0.917 Testing Accuracy: 0.852\n",
            "Iterations: 240 Training Accuracy: 0.912 Testing Accuracy: 0.8524\n",
            "Iterations: 250 Training Accuracy: 0.904 Testing Accuracy: 0.8531\n",
            "Iterations: 260 Training Accuracy: 0.914 Testing Accuracy: 0.854\n",
            "Iterations: 270 Training Accuracy: 0.915 Testing Accuracy: 0.855\n",
            "Iterations: 280 Training Accuracy: 0.917 Testing Accuracy: 0.8557\n",
            "Iterations: 290 Training Accuracy: 0.92 Testing Accuracy: 0.8562"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0"
      ],
      "metadata": {
        "id": "aFw30AFk9ozm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}